# projects section data
# If you don't have language feature(language.yml is empty), ignore "i18n" items
# Suggest projects' img be located at '/static/assets/img/landing', and edit following img items.

- name: Efficient 360 Video Streaming to Head-Mounted Displays in Virtual Reality
  i18n: 
  url: https://nmsl.cs.nthu.edu.tw/360video
  img: /static/assets/img/landing/watch_part_of_360video.jpg
  desc: - Design and develop a tile-based 360° video streaming system with viewers using HMDs<br>-Train and fine-tune new algorithms for 360° video streaming (e.g., viewed tile prediction algorithms)<br>- Collect a 360° video viewing Dataset, which contains content and sensor data, with viewers using HMDs<br>- New Quality-of-Experience (QoE) metrics designed for 360° video streaming to HMDs<br>- Optimal bitrate allocation algorithm for 360° video streaming<br>- Rate-Distortion (R-D) optimization for 360° video streaming to HMDs

- name: Low-bitrate Video Conferencing on Mobile Devices
  i18n: 
  url: 
  img:
  desc: High bandwidth requirement is one of the most challenged problems on multiparty video conferencing due to its high data rate. The mechanism of video conferencing application may reduce the video quality such as frame rate to lower the bandwidth requirement, which may lead to the degradation of quality of experience for users. Therefore, we try to (i) real-time transmit only the face features or (ii) caching previous representative frames at both sender and reciver side, instead of each complete frame during the video conferencing. A warping or matching mechanism is performed on previous frames or cached frames at the receiver side when receiving the facial features. This can reduce transmission bitrate largely while achieve high quality since users usually only move their head a little or change their facial expression during the video conferencing.

- name: Video Crowdsensing for Wearable Camera
  i18n: 
  url: 
  img:
  desc: Wearable and mobile devices are widely used for crowdsensing, as they come with many sensors and are carried everywhere. Among the sensing data, videos annotated with temporal-spatial metadata, contain huge amount of information, but consume too much precious storage space. We solve the problem of optimizing cloud-based video crowdsensing in three steps. First, we study the optimal transcoding problem on wearable and mobile cameras. We propose an algo- rithm to optimally select the coding parameters to fit more videos at higher quality on wearable and mobile cameras. Second, we empirically investigate the throughput of different file transfer protocols from wearable and mobile devices to cloud servers and propose a real-time algorithm to select the best protocol under diverse network conditions. Last, we look into the performance of cloud databases for sensor-annotated videos, and implement a practical algorithm to search videos overlapping with a target geographical region.

- name: Opensource Platform for Smart Lens
  i18n: 
  url: 
  img:
  desc: Smart lenses are detachable lenses connected to mobile devices via wireless networks, which are not constrained by the small form factor of mobile devices, and have potential to deliver better photo (video) quality. However, the viewfinder previews of smart lenses on mobile devices are difficult to optimize, due to the strict resource constraints on smart lenses and fluctuating wireless network conditions. We design, implement, and evaluate an open-source smart lens. It achieves three design goals: (i) cost effectiveness, (ii) low interaction latency, and (iii) high preview quality by: (i) selecting an embedded system board that is just powerful enough, (ii) minimizing per-component latency, and (iii) dynamically adapting the video coding parameters to maximizing Quality of Experience (QoE), respectively. We are currently extending our project to support 360 degree video for more immersive application.

- name: Interference-Aware Multi-Video Streaming Over Crowded ISM Bands
  i18n: 
  url: 
  img:
  desc: Fueled by the increasing popularity of computing devices, more and more people communicate, share, and collaborate via these devices anywhere and anytime. Screencast becomes a critical enabler for many applications, such as multi-party video conferencing, distance educations with multi-devices, and tele-medicine and remote nursing. Take multi-party video conferencing as an example, people prefer to merge the contents from IP cameras, local laptop computers, and remote desktops to a single projector or large display to facilitate discussions. We envision that such content sharing is done conveniently over a wireless network rather than cumbersome physical cables. However, concurrently transmitting multiple video streams over a single WiFi access point may lead to inferior Quality-of-Experience (QoE) due to degraded throughput and higher packet loss rates caused by network congestion and background interference. There- fore, we plan to leverage centralized controller and monitor nodes to collect traffic information and develop interference-aware WiFi bandwidth estimation algorithm to real-time measure the network condition. Afterwards, We are able to further perform interference mitigation, traffic scheduling, and resource allocation to improve user experience.
